{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/rhodes-byu/cs180-winter25/blob/main/labs/09-clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><p><b>After clicking the \"Open in Colab\" link, copy the notebook to your own Google Drive before getting started, or it will not save your work</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most straightforward tasks we can perform on a data set without labels is to find groups of data in our dataset which are similar to one another -- what we call clusters.\n",
    "\n",
    "K-Means is one of the most popular \"clustering\" algorithms. K-means stores k centroids that it uses to define clusters. A point is considered to be in a particular cluster if it is closer to that cluster's centroid than any other centroid.\n",
    "\n",
    "K-Means finds the best centroids by alternating between (1) assigning data points to clusters based on the current centroids (2) chosing centroids (points which are the center of a cluster) based on the current assignment of data points to clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Dataset: MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### MNIST Dataset: A Brief Overview\n",
    "\n",
    "#### Introduction\n",
    "The **MNIST (Modified National Institute of Standards and Technology) dataset** is one of the most widely used datasets in machine learning, particularly for benchmarking image classification algorithms.\n",
    "\n",
    "#### Description\n",
    "- **Type**: Handwritten digit dataset\n",
    "- **Size**: 70,000 grayscale images (28x28 pixels)\n",
    "  - **Training set**: 60,000 images\n",
    "  - **Test set**: 10,000 images\n",
    "- **Classes**: 10 (Digits 0-9)\n",
    "- **Format**: Each image is a 28Ã—28 matrix with pixel values ranging from 0 (black) to 255 (white).\n",
    "\n",
    "#### Why MNIST?\n",
    "- Simple yet challenging enough for ML research.\n",
    "- Well-structured and preprocessed, eliminating the need for extensive data cleaning.\n",
    "\n",
    "#### Accessing MNIST\n",
    "- Available in popular ML libraries:\n",
    "  - **TensorFlow/Keras**: `tf.keras.datasets.mnist`\n",
    "  - **PyTorch**: `torchvision.datasets.MNIST`\n",
    "  - **Scikit-learn**: `fetch_openml('mnist_784')`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version = 1, parser = 'auto')\n",
    "X, y = mnist['data'], mnist['target']\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST digits in this set are flattened arrays of 784 pixels. We can reshape them to 28x28 pixels and plot them using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x212051a6ad0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = X.iloc[0, :].values.reshape((28, 28))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparing the Data\n",
    "\n",
    "a. Using the full dataset, *normalize* each column so that the minimum column value is 0, and the maximum is 1.\n",
    "    (Hint: if your normalization process leads to missing values, replace these with 0!)\n",
    "\n",
    "b. Get a random sample of 10% of the data. The full dataset may take a while to run some of the below methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
      "0         NaN       0       0       0       0       0       0       0       0   \n",
      "1         NaN       0       0       0       0       0       0       0       0   \n",
      "2         NaN       0       0       0       0       0       0       0       0   \n",
      "3         NaN       0       0       0       0       0       0       0       0   \n",
      "4         NaN       0       0       0       0       0       0       0       0   \n",
      "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "69995     NaN       0       0       0       0       0       0       0       0   \n",
      "69996     NaN       0       0       0       0       0       0       0       0   \n",
      "69997     NaN       0       0       0       0       0       0       0       0   \n",
      "69998     NaN       0       0       0       0       0       0       0       0   \n",
      "69999     NaN       0       0       0       0       0       0       0       0   \n",
      "\n",
      "       pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
      "0            0  ...         0         0         0         0         0   \n",
      "1            0  ...         0         0         0         0         0   \n",
      "2            0  ...         0         0         0         0         0   \n",
      "3            0  ...         0         0         0         0         0   \n",
      "4            0  ...         0         0         0         0         0   \n",
      "...        ...  ...       ...       ...       ...       ...       ...   \n",
      "69995        0  ...         0         0         0         0         0   \n",
      "69996        0  ...         0         0         0         0         0   \n",
      "69997        0  ...         0         0         0         0         0   \n",
      "69998        0  ...         0         0         0         0         0   \n",
      "69999        0  ...         0         0         0         0         0   \n",
      "\n",
      "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
      "0             0         0         0         0         0  \n",
      "1             0         0         0         0         0  \n",
      "2             0         0         0         0         0  \n",
      "3             0         0         0         0         0  \n",
      "4             0         0         0         0         0  \n",
      "...         ...       ...       ...       ...       ...  \n",
      "69995         0         0         0         0         0  \n",
      "69996         0         0         0         0         0  \n",
      "69997         0         0         0         0         0  \n",
      "69998         0         0         0         0         0  \n",
      "69999         0         0         0         0         0  \n",
      "\n",
      "[70000 rows x 784 columns]\n",
      "      pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
      "0       None     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "1       None     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "2       None     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "3       None     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "4       None     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "69995   None     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "69996   None     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "69997   None     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "69998   None     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "69999   None     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "\n",
      "       pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
      "0          NaN  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "1          NaN  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "2          NaN  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "3          NaN  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "4          NaN  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "...        ...  ...       ...       ...       ...       ...       ...   \n",
      "69995      NaN  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "69996      NaN  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "69997      NaN  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "69998      NaN  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "69999      NaN  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "\n",
      "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
      "0           0.0       NaN       NaN       NaN       NaN  \n",
      "1           0.0       NaN       NaN       NaN       NaN  \n",
      "2           0.0       NaN       NaN       NaN       NaN  \n",
      "3           0.0       NaN       NaN       NaN       NaN  \n",
      "4           0.0       NaN       NaN       NaN       NaN  \n",
      "...         ...       ...       ...       ...       ...  \n",
      "69995       0.0       NaN       NaN       NaN       NaN  \n",
      "69996       0.0       NaN       NaN       NaN       NaN  \n",
      "69997       0.0       NaN       NaN       NaN       NaN  \n",
      "69998       0.0       NaN       NaN       NaN       NaN  \n",
      "69999       0.0       NaN       NaN       NaN       NaN  \n",
      "\n",
      "[70000 rows x 784 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_7392\\1428191224.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  normalized_df[\"pixel1\"] = normalized_df[\"pixel1\"].fillna(0, inplace = True)\n"
     ]
    }
   ],
   "source": [
    "# a.\n",
    "df = X\n",
    "print(df)\n",
    "normalized_df = (df-df.min())/(df.max()-df.min())\n",
    "normalized_df[\"pixel1\"] = normalized_df[\"pixel1\"].fillna(0, inplace = True)\n",
    "print(normalized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. K-Means Clustering\n",
    "\n",
    "a. Using the MNIST subset, determine the optimal k value for k-means according to the silhouette score. Use a range of k-values from 2 - 12. \n",
    "\n",
    "b. Fit a k-means model with the optimal k value.\n",
    "\n",
    "c. Using a dimensionality reduction method, $t$-SNE, generate a two-dimensional representation (called an **embedding**)of the MNIST subset. (See an example [here](https://scikit-learn.org/0.16/modules/generated/sklearn.manifold.TSNE.html). Use default values, but setting random_state = 42.)\n",
    "\n",
    "d. Create two side-by-side scatterplots using the $t$-SNE represenation using plt.subplots. Color the first fig according to the true labels and the second according to k-means cluster labels. Be sure to include proper figure titles and a legend.\n",
    "\n",
    "e. Describe the fit. Does this align with your expectation, given the silhouette score?\n",
    "\n",
    "f. Fit k-means with 10 clusters and calculate ten slihouette score. How does the slihouette score compare with the optimal k? (If 10 is optimal, just say so.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part e comments here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. comments here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Hierarchical Clustering\n",
    "Here you will be performing hierarchical clustering on the same data subset.\n",
    "\n",
    "Here you will be comparing linkage methods and metrics. Use the following linkage/metric combinations:\n",
    "\n",
    "| Linkage Method | Metrics              |\n",
    "|---------------|----------------------|\n",
    "| Ward         | Euclidean            |\n",
    "| Single       | Euclidean, Cosine, Manhattan |\n",
    "| Complete     | Euclidean, Cosine, Manhattan |\n",
    "| Average      | Euclidean, Cosine, Manhattan |\n",
    "\n",
    "a. Fit hierarhiccal clustering with 10 clusters for each combination and store the silhouette scores. \n",
    "\n",
    "b. According to the silhouette scores, which combination is optimal? Fit an HC model with this combination and store the cluster labels (make these a different variable name from that used for your k-means labels; we will eventually be comparing all clustering models).\n",
    "\n",
    "c. As in part 2. d., plot side-by-side scatterplots using the same $t$-SNE embedding.\n",
    "\n",
    "d. Describe the fit. Does this align with your expectation, given the silhouette score?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part d comments here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. DBSCAN Clustering\n",
    "\n",
    "DBSCAN can be a little tricky to tune due to its two hyperparameters, `eps` and `min_samples`. The `eps` parameter is the maximum distance between two samples for one to be considered as in the neighborhood of the other. The `min_samples` parameter is the number of samples in a neighborhood for a point to be considered as a core point. Here, we will attempt to find optimal hyperparameter combinations, focusing primarily on `eps`.\n",
    "\n",
    "a. Setting `min_samples`\n",
    "A rule of thumb is to set `min_samples` to be the number of dimensions (columns) in the dataset plus one. Here, you will try two different values:\n",
    "   1. Follow the rule of thumb.\n",
    "   2. Set `min_samples` to the number of observations in the sample divided by 10 (roughly equal numbers per digit label).\n",
    "\n",
    "For each `min_samples` value **k** (both cases):\n",
    "   - i. Calculate the distance between every data point and its **kth closest neighbor**.  \n",
    "   - ii. Compute the average **kth nearest neighbor** distances (for both `min_samples` values).  \n",
    "   - iii. Compute the standard deviation of the **kth nearest neighbor** distances (for both `min_samples` values).  \n",
    "   - iv. Generate a set of 10 `eps` values equally spaced between **mean Â± standard deviation** (for each `min_samples` value).  \n",
    "\n",
    "b. Using the sets of `min_samples` and `eps` values, apply DBSCAN and record all silhouette scores.\n",
    "\n",
    "c. Fit DBSCAN using the best hyperparameters based on the silhouette scores.\n",
    "\n",
    "d. Plot the $t$-SNE embedded values using subplots:\n",
    "   - One colored by **true labels**.\n",
    "   - Another colored by **cluster labels**.\n",
    "\n",
    "e. Describe the clustering results and comment on your findings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part e comments here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Adjusted Rand Index (ARI) in Clustering\n",
    "\n",
    "The **Adjusted Rand Index (ARI)** is a metric used to evaluate the similarity between two clusterings, accounting for chance. It measures how well a clustering algorithmâ€™s results match a known ground truth or reference clustering.\n",
    "\n",
    "#### **Intuition**\n",
    "- Suppose you have a set of data points, and you classify them into groups (clusters).\n",
    "- ARI compares your clustering to a \"correct\" classification and checks how often pairs of points are **correctly grouped together** or **correctly separated** in both cases.\n",
    "- A simple **Rand Index** gives a raw similarity score, but ARI adjusts for randomnessâ€”ensuring that random assignments donâ€™t get an artificially high score.\n",
    "\n",
    "#### **How It's Used**\n",
    "- **Benchmarking Clustering Algorithms:** ARI is useful for comparing different clustering methods to a gold-standard classification.\n",
    "- **No Bias Toward Number of Clusters:** Unlike some other metrics, ARI corrects for the number of clusters, making it more reliable when clusters differ in size or number.\n",
    "\n",
    "#### **Interpreting Results**\n",
    "- **Range:** ARI ranges from **-1 to 1**.  \n",
    "  - **1** = Perfect agreement with ground truth  \n",
    "  - **0** = Random clustering  \n",
    "  - **Negative values** = Worse than random (unlikely in practice)  \n",
    "- **Application:** Used in cases like image segmentation, document clustering, or biological data grouping, where a reference classification exists.\n",
    "\n",
    "### **Python Implementation**\n",
    "- **Import** via sklearn.metrics (the function `adjusted_rand_score`)\n",
    "- **Inputs**: True Labels, Predicted Labels (E.g., cluster labels)\n",
    "- **Output**: The Adjusted Rand Index Value\n",
    "\n",
    "\n",
    "### **Questions**:\n",
    "\n",
    "a. For each clustering method (k-means, hierarchical clustering, DBSCAN), calculate the adjusted rand index using the models with optimal hyperparameters.\n",
    "\n",
    "b. Plot all four scatterplots using subplots (1 row, 4 columns). The first colored by the true values, and the rest colored by your cluster labels (k-means, HC, DBSCAN).\n",
    "\n",
    "c. Do the scores seem reflecive of the assigned cluster labels, according to the plots?  Please explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part c comments here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Visualizing Clustered Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick one of the cluster labels from one of the clustering methods (your choice). Select a random subset of 20 points belonging to that cluster.  \n",
    "\n",
    "a. Using subplots (5 rows, 4 columns), plot the 20 digit images.  \n",
    "b. Comment on the clustered points (e.g., do they look similar?, are the representing the same digit?, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part b comments here:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
